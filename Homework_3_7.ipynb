{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP71FJPnaJ12wlcAnMAkvo2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jonathan-code-hub/MAT-422-Math-Methods-in-Data-Science/blob/main/Homework_3_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.7 Artifical Neural Networks"
      ],
      "metadata": {
        "id": "r0pFHuilfony"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.7.1 Mathematical Formulation"
      ],
      "metadata": {
        "id": "24Yul-Lqe2lN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61WJelQueOJS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "978f6e66-3b90-4ac9-c17a-4ff4f28fcbcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 1.071208645248812\n",
            "Epoch 1000, Loss: 0.6872738964694367\n",
            "Epoch 2000, Loss: 0.6333346669080255\n",
            "Epoch 3000, Loss: 0.4866074538371573\n",
            "Epoch 4000, Loss: 0.2746997457412422\n",
            "Epoch 5000, Loss: 0.15286127373626462\n",
            "Epoch 6000, Loss: 0.10629882664579617\n",
            "Epoch 7000, Loss: 0.08349518707257461\n",
            "Epoch 8000, Loss: 0.06997241498750328\n",
            "Epoch 9000, Loss: 0.060954064167824985\n",
            "\n",
            "Predictions after training:\n",
            "[[0.05586523]\n",
            " [0.94958945]\n",
            " [0.9486076 ]\n",
            " [0.05434364]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sigmoid activation function #\n",
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    Sigmoid function introduces non-linearity.\n",
        "    Formula: σ(x) = 1 / (1 + exp(-x))\n",
        "    \"\"\"\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Sigmoid derivative (for backpropagation) #\n",
        "def sigmoid_derivative(x):\n",
        "    \"\"\"\n",
        "    Derivative of sigmoid function.\n",
        "    Formula: σ'(x) = σ(x) * (1 - σ(x))\n",
        "    \"\"\"\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Binary cross-entropy loss function #\n",
        "def binary_crossentropy(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Binary cross-entropy loss function.\n",
        "    Formula: - (y_true * log(y_pred) + (1 - y_true) * log(1 - y_pred))\n",
        "    \"\"\"\n",
        "    epsilon = 1e-15  # To avoid log(0) #\n",
        "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Clipping to prevent log(0) #\n",
        "    return - (y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)).mean()\n",
        "\n",
        "# Neural Network Class #\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        \"\"\"\n",
        "        Initialize the weights and biases.\n",
        "        Formula for weights: w_ij for weights between neurons i and j\n",
        "        Formula for biases: b for each layer\n",
        "        \"\"\"\n",
        "\n",
        "        # Initialize weights and biases #\n",
        "        self.weights_input_hidden = np.random.rand(input_size, hidden_size)  # Weights between input and hidden layers #\n",
        "        self.weights_hidden_output = np.random.rand(hidden_size, output_size)  # Weights between hidden and output layers #\n",
        "        self.bias_hidden = np.zeros((1, hidden_size))  # Bias for hidden layer #\n",
        "        self.bias_output = np.zeros((1, output_size))  # Bias for output layer #\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward pass calculates activations layer by layer.\n",
        "        Formula: Z = X * W + b, where W is weight matrix and b is bias\n",
        "        \"\"\"\n",
        "        # Calculate the input to hidden layer ##\n",
        "        self.hidden_input = np.dot(X, self.weights_input_hidden) + self.bias_hidden\n",
        "        self.hidden_output = sigmoid(self.hidden_input)  # Apply sigmoid activation #\n",
        "\n",
        "        # Calculate the input to the output layer #\n",
        "        self.output_input = np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output\n",
        "        self.output = sigmoid(self.output_input)  # Apply sigmoid activation for binary output #\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, X, y, learning_rate):\n",
        "        \"\"\"\n",
        "        Backpropagation to update weights and biases.\n",
        "        Uses gradient descent to update weights.\n",
        "        \"\"\"\n",
        "        # Calculate the error at output layer #\n",
        "        output_error = self.output - y\n",
        "        output_delta = output_error * sigmoid_derivative(self.output)\n",
        "\n",
        "        # Calculate the error at hidden layer #\n",
        "        hidden_error = output_delta.dot(self.weights_hidden_output.T)\n",
        "        hidden_delta = hidden_error * sigmoid_derivative(self.hidden_output)\n",
        "\n",
        "        # Update weights and biases using gradient descent #\n",
        "        self.weights_input_hidden -= X.T.dot(hidden_delta) * learning_rate\n",
        "        self.weights_hidden_output -= self.hidden_output.T.dot(output_delta) * learning_rate\n",
        "        self.bias_hidden -= np.sum(hidden_delta, axis=0, keepdims=True) * learning_rate\n",
        "        self.bias_output -= np.sum(output_delta, axis=0, keepdims=True) * learning_rate\n",
        "\n",
        "    def train(self, X, y, epochs, learning_rate):\n",
        "        \"\"\"\n",
        "        Train the neural network by performing forward and backward passes.\n",
        "        The goal is to minimize the loss function over multiple epochs.\n",
        "        \"\"\"\n",
        "        # Train the neural network #\n",
        "        for epoch in range(epochs):\n",
        "            self.forward(X)\n",
        "            self.backward(X, y, learning_rate)\n",
        "\n",
        "            if epoch % 1000 == 0:\n",
        "                loss = binary_crossentropy(y, self.output)\n",
        "                print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "# Testing #\n",
        "\n",
        "# Training data: XOR problem #\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Input #\n",
        "y = np.array([[0], [1], [1], [0]])  # Output (XOR) #\n",
        "\n",
        "# Initialize and train the neural network #\n",
        "nn = NeuralNetwork(input_size=2, hidden_size=4, output_size=1)  # 2 inputs, 4 hidden neurons, 1 output neuron #\n",
        "nn.train(X, y, epochs=10000, learning_rate=0.1)\n",
        "\n",
        "# Test the trained network #\n",
        "predictions = nn.forward(X)\n",
        "print(\"\\nPredictions after training:\")\n",
        "print(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.7.2 Activation Functions"
      ],
      "metadata": {
        "id": "HElkM1rrezOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sigmoid activation function #\n",
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    # Sigmoid activation function: maps input between 0 and 1 #\n",
        "    Formula: σ(x) = 1 / (1 + exp(-x))\n",
        "    \"\"\"\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Sigmoid derivative #\n",
        "def sigmoid_derivative(x):\n",
        "    \"\"\"\n",
        "    # Derivative of Sigmoid function, used in backpropagation #\n",
        "    Formula: σ'(x) = σ(x) * (1 - σ(x))\n",
        "    \"\"\"\n",
        "    return x * (1 - x)\n",
        "\n",
        "# ReLU activation function #\n",
        "def relu(x):\n",
        "    \"\"\"\n",
        "    # ReLU activation function: maps negative values to 0 #\n",
        "    Formula: ReLU(x) = max(0, x)\n",
        "    \"\"\"\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "# ReLU derivative #\n",
        "def relu_derivative(x):\n",
        "    \"\"\"\n",
        "    # Derivative of ReLU function, used in backpropagation #\n",
        "    Formula: ReLU'(x) = 1 for x > 0 else 0\n",
        "    \"\"\"\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "# Tanh activation function #\n",
        "def tanh(x):\n",
        "    \"\"\"\n",
        "    # Tanh activation function: maps input between -1 and 1 #\n",
        "    Formula: tanh(x) = (e^x - e^-x) / (e^x + e^-x)\n",
        "    \"\"\"\n",
        "    return np.tanh(x)\n",
        "\n",
        "# Tanh derivative #\n",
        "def tanh_derivative(x):\n",
        "    \"\"\"\n",
        "    # Derivative of Tanh function, used in backpropagation #\n",
        "    Formula: tanh'(x) = 1 - tanh(x)^2\n",
        "    \"\"\"\n",
        "    return 1 - np.tanh(x)**2\n",
        "\n",
        "# Softmax activation function (used for multi-class classification) #\n",
        "def softmax(x):\n",
        "    \"\"\"\n",
        "    # Softmax activation function: converts logits to probabilities for multi-class classification #\n",
        "    Formula: softmax(x_i) = exp(x_i) / sum(exp(x_j) for all j)\n",
        "    \"\"\"\n",
        "    exp_values = np.exp(x - np.max(x))  # Shift for numerical stability\n",
        "    return exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
        "\n",
        "# Example Neural Network with ReLU as activation function #\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size, activation_function):\n",
        "        \"\"\"\n",
        "        # Initialize the neural network with given sizes and selected activation function #\n",
        "        \"\"\"\n",
        "        # Random initialization of weights and biases #\n",
        "        self.weights_input_hidden = np.random.rand(input_size, hidden_size)  # Weights between input and hidden layers #\n",
        "        self.weights_hidden_output = np.random.rand(hidden_size, output_size)  # Weights between hidden and output layers #\n",
        "        self.bias_hidden = np.zeros((1, hidden_size))  # Bias for hidden layer #\n",
        "        self.bias_output = np.zeros((1, output_size))  # Bias for output layer #\n",
        "\n",
        "        # Choose the activation function #\n",
        "        self.activation_function = activation_function\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        # Forward pass through the network #\n",
        "        \"\"\"\n",
        "        # Input to hidden layer #\n",
        "        self.hidden_input = np.dot(X, self.weights_input_hidden) + self.bias_hidden #\n",
        "        if self.activation_function == 'sigmoid':\n",
        "            self.hidden_output = sigmoid(self.hidden_input)  # Apply sigmoid #\n",
        "        elif self.activation_function == 'relu':\n",
        "            self.hidden_output = relu(self.hidden_input)  # Apply ReLU #\n",
        "        elif self.activation_function == 'tanh':\n",
        "            self.hidden_output = tanh(self.hidden_input)  # Apply Tanh #\n",
        "\n",
        "        # Hidden to output layer #\n",
        "        self.output_input = np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output #\n",
        "        if self.activation_function == 'sigmoid':\n",
        "            self.output = sigmoid(self.output_input)  # Apply sigmoid #\n",
        "        elif self.activation_function == 'relu':\n",
        "            self.output = relu(self.output_input)  # Apply ReLU #\n",
        "        elif self.activation_function == 'tanh':\n",
        "            self.output = tanh(self.output_input)  # Apply Tanh #\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, X, y, learning_rate):\n",
        "        \"\"\"\n",
        "        # Backpropagation to adjust weights and biases #\n",
        "        \"\"\"\n",
        "        # Compute the error at the output layer #\n",
        "        output_error = self.output - y #\n",
        "        if self.activation_function == 'sigmoid':\n",
        "            output_delta = output_error * sigmoid_derivative(self.output)  # Sigmoid derivative #\n",
        "        elif self.activation_function == 'relu':\n",
        "            output_delta = output_error * relu_derivative(self.output)  # ReLU derivative #\n",
        "        elif self.activation_function == 'tanh':\n",
        "            output_delta = output_error * tanh_derivative(self.output)  # Tanh derivative #\n",
        "\n",
        "        # Compute the error at the hidden layer #\n",
        "        hidden_error = output_delta.dot(self.weights_hidden_output.T) #\n",
        "        if self.activation_function == 'sigmoid':\n",
        "            hidden_delta = hidden_error * sigmoid_derivative(self.hidden_output)  # Sigmoid derivative #\n",
        "        elif self.activation_function == 'relu':\n",
        "            hidden_delta = hidden_error * relu_derivative(self.hidden_output)  # ReLU derivative #\n",
        "        elif self.activation_function == 'tanh':\n",
        "            hidden_delta = hidden_error * tanh_derivative(self.hidden_output)  # Tanh derivative #\n",
        "\n",
        "        # Update weights and biases using gradient descent #\n",
        "        self.weights_input_hidden -= X.T.dot(hidden_delta) * learning_rate #\n",
        "        self.weights_hidden_output -= self.hidden_output.T.dot(output_delta) * learning_rate #\n",
        "        self.bias_hidden -= np.sum(hidden_delta, axis=0, keepdims=True) * learning_rate #\n",
        "        self.bias_output -= np.sum(output_delta, axis=0, keepdims=True) * learning_rate #\n",
        "\n",
        "    def train(self, X, y, epochs, learning_rate):\n",
        "        \"\"\"\n",
        "        # Train the neural network using forward and backward passes #\n",
        "        \"\"\"\n",
        "        for epoch in range(epochs):\n",
        "            self.forward(X) #\n",
        "            self.backward(X, y, learning_rate) #\n",
        "\n",
        "            if epoch % 1000 == 0:\n",
        "                loss = binary_crossentropy(y, self.output)  # Loss function to calculate how well the model is performing #\n",
        "                print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "# Example usage with the XOR problem with ReLU activation:\n",
        "\n",
        "# Training data: XOR problem #\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Input #\n",
        "y = np.array([[0], [1], [1], [0]])  # Output (XOR) #\n",
        "\n",
        "# Initialize and train the neural network with ReLU activation #\n",
        "nn = NeuralNetwork(input_size=2, hidden_size=4, output_size=1, activation_function='relu')  # 2 inputs, 4 hidden neurons, 1 output neuron #\n",
        "nn.train(X, y, epochs=10000, learning_rate=0.1) #\n",
        "\n",
        "# Test the trained network #\n",
        "predictions = nn.forward(X) #\n",
        "print(\"\\nPredictions after training:\")\n",
        "print(predictions)\n"
      ],
      "metadata": {
        "id": "nbGVOoYkezmz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69461958-85d4-4841-a183-b5c519ad983e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 8.69008163154445\n",
            "Epoch 1000, Loss: 9.992007221626415e-16\n",
            "Epoch 2000, Loss: 9.992007221626415e-16\n",
            "Epoch 3000, Loss: 9.992007221626415e-16\n",
            "Epoch 4000, Loss: 9.992007221626415e-16\n",
            "Epoch 5000, Loss: 9.992007221626415e-16\n",
            "Epoch 6000, Loss: 9.992007221626415e-16\n",
            "Epoch 7000, Loss: 9.992007221626415e-16\n",
            "Epoch 8000, Loss: 9.992007221626415e-16\n",
            "Epoch 9000, Loss: 9.992007221626415e-16\n",
            "\n",
            "Predictions after training:\n",
            "[[4.45549659e-16]\n",
            " [1.00000000e+00]\n",
            " [1.00000000e+00]\n",
            " [3.60450649e-16]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.7.3 Cost Function"
      ],
      "metadata": {
        "id": "CMU9d26Ae0Lm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Mean Squared Error (MSE) cost function #\n",
        "def mean_squared_error(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    # Mean Squared Error (MSE) cost function: measures the average squared difference between the predicted and actual values #\n",
        "    Formula: MSE = (1/n) * sum((y_true - y_pred)^2)\n",
        "    \"\"\"\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "# Binary Cross-Entropy (Log Loss) cost function #\n",
        "def binary_crossentropy(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    # Binary Cross-Entropy cost function: used for binary classification tasks #\n",
        "    Formula: BCE = -(y_true * log(y_pred) + (1 - y_true) * log(1 - y_pred))\n",
        "    \"\"\"\n",
        "    epsilon = 1e-15  # To avoid log(0)\n",
        "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Clip values to prevent log(0)\n",
        "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "\n",
        "# Categorical Cross-Entropy cost function #\n",
        "def categorical_crossentropy(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    # Categorical Cross-Entropy cost function: used for multi-class classification tasks #\n",
        "    Formula: CCE = -sum(y_true * log(y_pred))\n",
        "    \"\"\"\n",
        "    epsilon = 1e-15  # To avoid log(0)\n",
        "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Clip values to prevent log(0)\n",
        "    return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n",
        "\n",
        "# Example Neural Network using MSE as cost function #\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size, activation_function, cost_function):\n",
        "        \"\"\"\n",
        "        # Initialize the neural network with given sizes, selected activation function, and cost function #\n",
        "        \"\"\"\n",
        "        # Random initialization of weights and biases #\n",
        "        self.weights_input_hidden = np.random.rand(input_size, hidden_size)  # Weights between input and hidden layers #\n",
        "        self.weights_hidden_output = np.random.rand(hidden_size, output_size)  # Weights between hidden and output layers #\n",
        "        self.bias_hidden = np.zeros((1, hidden_size))  # Bias for hidden layer #\n",
        "        self.bias_output = np.zeros((1, output_size))  # Bias for output layer #\n",
        "\n",
        "        # Choose the activation function and cost function #\n",
        "        self.activation_function = activation_function\n",
        "        self.cost_function = cost_function\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        # Forward pass through the network #\n",
        "        \"\"\"\n",
        "        # Input to hidden layer #\n",
        "        self.hidden_input = np.dot(X, self.weights_input_hidden) + self.bias_hidden #\n",
        "        if self.activation_function == 'sigmoid':\n",
        "            self.hidden_output = sigmoid(self.hidden_input)  # Apply sigmoid #\n",
        "        elif self.activation_function == 'relu':\n",
        "            self.hidden_output = relu(self.hidden_input)  # Apply ReLU #\n",
        "        elif self.activation_function == 'tanh':\n",
        "            self.hidden_output = tanh(self.hidden_input)  # Apply Tanh #\n",
        "\n",
        "        # Hidden to output layer #\n",
        "        self.output_input = np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output #\n",
        "        if self.activation_function == 'sigmoid':\n",
        "            self.output = sigmoid(self.output_input)  # Apply sigmoid #\n",
        "        elif self.activation_function == 'relu':\n",
        "            self.output = relu(self.output_input)  # Apply ReLU #\n",
        "        elif self.activation_function == 'tanh':\n",
        "            self.output = tanh(self.output_input)  # Apply Tanh #\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, X, y, learning_rate):\n",
        "        \"\"\"\n",
        "        # Backpropagation to adjust weights and biases #\n",
        "        \"\"\"\n",
        "        # Compute the error at the output layer #\n",
        "        output_error = self.output - y #\n",
        "        if self.activation_function == 'sigmoid':\n",
        "            output_delta = output_error * sigmoid_derivative(self.output)  # Sigmoid derivative #\n",
        "        elif self.activation_function == 'relu':\n",
        "            output_delta = output_error * relu_derivative(self.output)  # ReLU derivative #\n",
        "        elif self.activation_function == 'tanh':\n",
        "            output_delta = output_error * tanh_derivative(self.output)  # Tanh derivative #\n",
        "\n",
        "        # Compute the error at the hidden layer #\n",
        "        hidden_error = output_delta.dot(self.weights_hidden_output.T) #\n",
        "        if self.activation_function == 'sigmoid':\n",
        "            hidden_delta = hidden_error * sigmoid_derivative(self.hidden_output)  # Sigmoid derivative #\n",
        "        elif self.activation_function == 'relu':\n",
        "            hidden_delta = hidden_error * relu_derivative(self.hidden_output)  # ReLU derivative #\n",
        "        elif self.activation_function == 'tanh':\n",
        "            hidden_delta = hidden_error * tanh_derivative(self.hidden_output)  # Tanh derivative #\n",
        "\n",
        "        # Update weights and biases using gradient descent #\n",
        "        self.weights_input_hidden -= X.T.dot(hidden_delta) * learning_rate #\n",
        "        self.weights_hidden_output -= self.hidden_output.T.dot(output_delta) * learning_rate #\n",
        "        self.bias_hidden -= np.sum(hidden_delta, axis=0, keepdims=True) * learning_rate #\n",
        "        self.bias_output -= np.sum(output_delta, axis=0, keepdims=True) * learning_rate #\n",
        "\n",
        "    def train(self, X, y, epochs, learning_rate):\n",
        "        \"\"\"\n",
        "        # Train the neural network using forward and backward passes #\n",
        "        \"\"\"\n",
        "        for epoch in range(epochs):\n",
        "            self.forward(X) #\n",
        "            self.backward(X, y, learning_rate) #\n",
        "\n",
        "            # Calculate and print the cost (loss) for every epoch #\n",
        "            if epoch % 1000 == 0:\n",
        "                cost = self.cost_function(y, self.output)  # Use the selected cost function #\n",
        "                print(f\"Epoch {epoch}, Cost: {cost}\")\n",
        "\n",
        "# Example usage for XOR problem with MSE cost function:\n",
        "\n",
        "# Training data: XOR problem #\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Input #\n",
        "y = np.array([[0], [1], [1], [0]])  # Output (XOR) #\n",
        "\n",
        "# Initialize and train the neural network with MSE cost function #\n",
        "nn = NeuralNetwork(input_size=2, hidden_size=4, output_size=1, activation_function='relu', cost_function=mean_squared_error)  # 2 inputs, 4 hidden neurons, 1 output neuron #\n",
        "nn.train(X, y, epochs=10000, learning_rate=0.1) #\n",
        "\n",
        "# Test the trained network #\n",
        "predictions = nn.forward(X) #\n",
        "print(\"\\nPredictions after training:\")\n",
        "print(predictions)\n"
      ],
      "metadata": {
        "id": "aBbZbwRve0kk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8660782e-5a0d-48b5-e23a-32757c482677"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Cost: 0.5319846010373317\n",
            "Epoch 1000, Cost: 3.998820270181945e-31\n",
            "Epoch 2000, Cost: 4.027413032227868e-31\n",
            "Epoch 3000, Cost: 4.027413032227868e-31\n",
            "Epoch 4000, Cost: 4.027413032227869e-31\n",
            "Epoch 5000, Cost: 4.027413032227868e-31\n",
            "Epoch 6000, Cost: 4.027413032227868e-31\n",
            "Epoch 7000, Cost: 4.027413032227868e-31\n",
            "Epoch 8000, Cost: 4.027413032227868e-31\n",
            "Epoch 9000, Cost: 4.027413032227868e-31\n",
            "\n",
            "Predictions after training:\n",
            "[[8.48700474e-16]\n",
            " [1.00000000e+00]\n",
            " [1.00000000e+00]\n",
            " [3.72544853e-16]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.7.4 Backpropagation"
      ],
      "metadata": {
        "id": "QYxNb1MpfGWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sigmoid activation function #\n",
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    # Sigmoid activation function: maps input between 0 and 1 #\n",
        "    Formula: σ(x) = 1 / (1 + exp(-x))\n",
        "    \"\"\"\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Sigmoid derivative #\n",
        "def sigmoid_derivative(x):\n",
        "    \"\"\"\n",
        "    # Sigmoid derivative function: used in backpropagation #\n",
        "    Formula: σ'(x) = σ(x) * (1 - σ(x))\n",
        "    \"\"\"\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Mean Squared Error (MSE) cost function #\n",
        "def mean_squared_error(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    # Mean Squared Error (MSE) cost function: measures the average squared difference between the predicted and actual values #\n",
        "    Formula: MSE = (1/n) * sum((y_true - y_pred)^2)\n",
        "    \"\"\"\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "# Neural Network class using Backpropagation #\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size, learning_rate):\n",
        "        \"\"\"\n",
        "        # Initialize the neural network with given sizes and learning rate #\n",
        "        \"\"\"\n",
        "        # Random initialization of weights and biases #\n",
        "        self.weights_input_hidden = np.random.rand(input_size, hidden_size)  # Weights between input and hidden layers #\n",
        "        self.weights_hidden_output = np.random.rand(hidden_size, output_size)  # Weights between hidden and output layers #\n",
        "        self.bias_hidden = np.zeros((1, hidden_size))  # Bias for hidden layer #\n",
        "        self.bias_output = np.zeros((1, output_size))  # Bias for output layer #\n",
        "        self.learning_rate = learning_rate  # Learning rate #\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        # Forward pass through the network #\n",
        "        \"\"\"\n",
        "        # Input to hidden layer #\n",
        "        self.hidden_input = np.dot(X, self.weights_input_hidden) + self.bias_hidden #\n",
        "        self.hidden_output = sigmoid(self.hidden_input)  # Apply sigmoid activation function #\n",
        "\n",
        "        # Hidden to output layer #\n",
        "        self.output_input = np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output #\n",
        "        self.output = sigmoid(self.output_input)  # Apply sigmoid activation function #\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, X, y):\n",
        "        \"\"\"\n",
        "        # Backpropagation to adjust weights and biases #\n",
        "        \"\"\"\n",
        "        # Compute the error at the output layer #\n",
        "        output_error = self.output - y #\n",
        "        output_delta = output_error * sigmoid_derivative(self.output)  # Apply the derivative of the sigmoid function #\n",
        "\n",
        "        # Compute the error at the hidden layer #\n",
        "        hidden_error = output_delta.dot(self.weights_hidden_output.T) #\n",
        "        hidden_delta = hidden_error * sigmoid_derivative(self.hidden_output)  # Apply the derivative of the sigmoid function #\n",
        "\n",
        "        # Update weights and biases using gradient descent #\n",
        "        self.weights_input_hidden -= X.T.dot(hidden_delta) * self.learning_rate # Update weights between input and hidden layers #\n",
        "        self.weights_hidden_output -= self.hidden_output.T.dot(output_delta) * self.learning_rate # Update weights between hidden and output layers #\n",
        "        self.bias_hidden -= np.sum(hidden_delta, axis=0, keepdims=True) * self.learning_rate # Update bias for hidden layer #\n",
        "        self.bias_output -= np.sum(output_delta, axis=0, keepdims=True) * self.learning_rate # Update bias for output layer #\n",
        "\n",
        "    def train(self, X, y, epochs):\n",
        "        \"\"\"\n",
        "        # Train the neural network using forward and backward passes (backpropagation) #\n",
        "        \"\"\"\n",
        "        for epoch in range(epochs):\n",
        "            self.forward(X)  # Perform the forward pass #\n",
        "            self.backward(X, y)  # Perform the backward pass (backpropagation) #\n",
        "\n",
        "            if epoch % 1000 == 0:\n",
        "                cost = mean_squared_error(y, self.output)  # Compute the cost (loss) using MSE #\n",
        "                print(f\"Epoch {epoch}, Cost: {cost}\")\n",
        "\n",
        "# Example usage for XOR problem with backpropagation:\n",
        "\n",
        "# Training data: XOR problem #\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Input #\n",
        "y = np.array([[0], [1], [1], [0]])  # Output (XOR) #\n",
        "\n",
        "# Initialize the neural network with 2 inputs, 4 hidden neurons, and 1 output neuron #\n",
        "nn = NeuralNetwork(input_size=2, hidden_size=4, output_size=1, learning_rate=0.1)\n",
        "\n",
        "# Train the neural network for 10000 epochs #\n",
        "nn.train(X, y, epochs=10000)\n",
        "\n",
        "# Test the trained network #\n",
        "predictions = nn.forward(X)  # Get predictions after training #\n",
        "print(\"\\nPredictions after training:\")\n",
        "print(predictions)\n"
      ],
      "metadata": {
        "id": "kdLMa1qCfISZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ac6bd90-7ccf-4049-d65c-11e08c79b17d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Cost: 0.3608962283912063\n",
            "Epoch 1000, Cost: 0.24938002718812224\n",
            "Epoch 2000, Cost: 0.24414817447705864\n",
            "Epoch 3000, Cost: 0.2052733394704204\n",
            "Epoch 4000, Cost: 0.12705484851328977\n",
            "Epoch 5000, Cost: 0.039235449659557727\n",
            "Epoch 6000, Cost: 0.015241735558838017\n",
            "Epoch 7000, Cost: 0.008408863888079824\n",
            "Epoch 8000, Cost: 0.00556349673451386\n",
            "Epoch 9000, Cost: 0.004074358333295972\n",
            "\n",
            "Predictions after training:\n",
            "[[0.05979164]\n",
            " [0.94586745]\n",
            " [0.94599085]\n",
            " [0.05737543]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.7.5 Backpropagation Algorithm"
      ],
      "metadata": {
        "id": "krYDb5U0fC6s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Initialize the weights and biases randomly\n",
        "2. For each epoch:\n",
        "   3. For each training example (input x, true output y):\n",
        "      4. Perform a forward pass:\n",
        "         - Calculate the activations for each layer of the network\n",
        "      5. Calculate the loss between the predicted output and the true output\n",
        "      6. Perform a backward pass:\n",
        "         - Compute the gradients for each layer's weights and biases\n",
        "         - Propagate the error back through the network\n",
        "      7. Update the weights and biases using the gradients and the learning rate\n",
        "3. Repeat steps 2-3 for a specified number of epochs"
      ],
      "metadata": {
        "id": "CozRZ1J8rP03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sigmoid activation function #\n",
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    # Sigmoid activation function: maps input between 0 and 1 #\n",
        "    Formula: σ(x) = 1 / (1 + exp(-x))\n",
        "    \"\"\"\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Sigmoid derivative function #\n",
        "def sigmoid_derivative(x):\n",
        "    \"\"\"\n",
        "    # Sigmoid derivative function: used in backpropagation #\n",
        "    Formula: σ'(x) = σ(x) * (1 - σ(x))\n",
        "    \"\"\"\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Mean Squared Error (MSE) cost function #\n",
        "def mean_squared_error(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    # Mean Squared Error (MSE) cost function: measures the average squared difference between the predicted and actual values #\n",
        "    Formula: MSE = (1/n) * sum((y_true - y_pred)^2)\n",
        "    \"\"\"\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "# Neural Network class with Backpropagation #\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size, learning_rate):\n",
        "        \"\"\"\n",
        "        # Initialize the neural network with given sizes and learning rate #\n",
        "        \"\"\"\n",
        "        # Random initialization of weights and biases #\n",
        "        self.weights_input_hidden = np.random.rand(input_size, hidden_size)  # Weights between input and hidden layers #\n",
        "        self.weights_hidden_output = np.random.rand(hidden_size, output_size)  # Weights between hidden and output layers #\n",
        "        self.bias_hidden = np.zeros((1, hidden_size))  # Bias for hidden layer #\n",
        "        self.bias_output = np.zeros((1, output_size))  # Bias for output layer #\n",
        "        self.learning_rate = learning_rate  # Learning rate #\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        # Forward pass through the network #\n",
        "        \"\"\"\n",
        "        # Input to hidden layer #\n",
        "        self.hidden_input = np.dot(X, self.weights_input_hidden) + self.bias_hidden #\n",
        "        self.hidden_output = sigmoid(self.hidden_input)  # Apply sigmoid activation function #\n",
        "\n",
        "        # Hidden to output layer #\n",
        "        self.output_input = np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output #\n",
        "        self.output = sigmoid(self.output_input)  # Apply sigmoid activation function #\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, X, y):\n",
        "        \"\"\"\n",
        "        # Backpropagation to adjust weights and biases #\n",
        "        \"\"\"\n",
        "        # Compute the error at the output layer #\n",
        "        output_error = self.output - y #\n",
        "        output_delta = output_error * sigmoid_derivative(self.output)  # Apply the derivative of the sigmoid function #\n",
        "\n",
        "        # Compute the error at the hidden layer #\n",
        "        hidden_error = output_delta.dot(self.weights_hidden_output.T) #\n",
        "        hidden_delta = hidden_error * sigmoid_derivative(self.hidden_output)  # Apply the derivative of the sigmoid function #\n",
        "\n",
        "        # Update weights and biases using gradient descent #\n",
        "        self.weights_input_hidden -= X.T.dot(hidden_delta) * self.learning_rate # Update weights between input and hidden layers #\n",
        "        self.weights_hidden_output -= self.hidden_output.T.dot(output_delta) * self.learning_rate # Update weights between hidden and output layers #\n",
        "        self.bias_hidden -= np.sum(hidden_delta, axis=0, keepdims=True) * self.learning_rate # Update bias for hidden layer #\n",
        "        self.bias_output -= np.sum(output_delta, axis=0, keepdims=True) * self.learning_rate # Update bias for output layer #\n",
        "\n",
        "    def train(self, X, y, epochs):\n",
        "        \"\"\"\n",
        "        # Train the neural network using forward and backward passes (backpropagation) #\n",
        "        \"\"\"\n",
        "        for epoch in range(epochs):\n",
        "            self.forward(X)  # Perform the forward pass #\n",
        "            self.backward(X, y)  # Perform the backward pass (backpropagation) #\n",
        "\n",
        "            # Calculate and print the cost (loss) every 1000 epochs #\n",
        "            if epoch % 1000 == 0:\n",
        "                cost = mean_squared_error(y, self.output)  # Compute the cost using MSE #\n",
        "                print(f\"Epoch {epoch}, Cost: {cost}\")\n",
        "\n",
        "# Example usage for XOR problem with backpropagation:\n",
        "\n",
        "# Training data: XOR problem #\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Input #\n",
        "y = np.array([[0], [1], [1], [0]])  # Output (XOR) #\n",
        "\n",
        "# Initialize the neural network with 2 inputs, 4 hidden neurons, and 1 output neuron #\n",
        "nn = NeuralNetwork(input_size=2, hidden_size=4, output_size=1, learning_rate=0.1)\n",
        "\n",
        "# Train the neural network for 10000 epochs #\n",
        "nn.train(X, y, epochs=10000)\n",
        "\n",
        "# Test the trained network #\n",
        "predictions = nn.forward(X)  # Get predictions after training #\n",
        "print(\"\\nPredictions after training:\")\n",
        "print(predictions)\n"
      ],
      "metadata": {
        "id": "Zqk4vIXze1Ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55c9045a-4fa6-4fc9-a826-bbb54f7d2bec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Cost: 0.3266735875006339\n",
            "Epoch 1000, Cost: 0.24796296082765684\n",
            "Epoch 2000, Cost: 0.22607733967232335\n",
            "Epoch 3000, Cost: 0.15680648954609744\n",
            "Epoch 4000, Cost: 0.06357736031223811\n",
            "Epoch 5000, Cost: 0.022064028899517127\n",
            "Epoch 6000, Cost: 0.010947610937981349\n",
            "Epoch 7000, Cost: 0.00680689301708027\n",
            "Epoch 8000, Cost: 0.004794309559946383\n",
            "Epoch 9000, Cost: 0.0036421719184096613\n",
            "\n",
            "Predictions after training:\n",
            "[[0.06082689]\n",
            " [0.94913223]\n",
            " [0.94935672]\n",
            " [0.05275093]]\n"
          ]
        }
      ]
    }
  ]
}